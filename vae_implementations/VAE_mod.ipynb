{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.21\n",
      "  Downloading numpy-1.21.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 8.8 MB/s eta 0:00:01MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.21.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Input, Flatten,\\\n",
    "Conv2DTranspose, BatchNormalization, LeakyReLU, Reshape, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset-\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training and testing sets are:\n",
      "X_train.shape: (50000, 32, 32, 3) & X_test.shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Specify hyper-parameters-\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "\n",
    "# Convert datasets to floating point types-\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize the training and testing datasets-\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "print(\"\\nDimensions of training and testing sets are:\")\n",
    "print(f\"X_train.shape: {X_train.shape} & X_test.shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_outlier_detector = True\n",
    "filepath = '/root/.keras/datasets/'\n",
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 16, 64)   3136        ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 8, 8, 128)    131200      ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 4, 4, 512)    1049088     ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 8192)         0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2)            16386       ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            6           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            6           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,199,822\n",
      "Trainable params: 1,199,822\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8192)              24576     \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 8, 8, 128)        1048704   \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 16, 16, 64)       131136    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 32, 32, 3)        3075      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,207,491\n",
      "Trainable params: 1,207,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoder_inputs = keras.Input(shape=(32, 32, 3))\n",
    "# x = Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu)(encoder_inputs)\n",
    "# x = Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "# x = Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(latent_dim, activation=\"relu\")(x)\n",
    "# z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "# z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "# z = Sampling()([z_mean, z_log_var])\n",
    "# encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "# encoder.summary()\n",
    "\n",
    "# latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "# x = layers.Dense(4*4*512)(latent_inputs)\n",
    "# x = layers.Reshape(target_shape=(4, 4, 512))(x)\n",
    "# x = Conv2DTranspose(128, 4, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "# x = Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "# decoder_outputs = Conv2DTranspose(3, 4, strides=2, padding='same', activation='sigmoid')(x)\n",
    "# decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "# decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 32)   896         ['input_24[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 8, 8, 64)     18496       ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)           (None, 4096)         0           ['conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 2)            8194        ['flatten_11[0][0]']             \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            6           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            6           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " sampling_11 (Sampling)         (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27,598\n",
      "Trainable params: 27,598\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_25 (InputLayer)       [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 4096)              12288     \n",
      "                                                                 \n",
      " reshape_11 (Reshape)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_25 (Conv2D  (None, 16, 16, 32)       18464     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_26 (Conv2D  (None, 32, 32, 3)        867       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,619\n",
      "Trainable params: 31,619\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MODIFIED\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = Conv2D(32, 3, strides=2, padding='same', activation=tf.nn.relu)(encoder_inputs)\n",
    "x = Conv2D(64, 3, strides=2,  padding='same', activation=tf.nn.relu)(x)\n",
    "# x = Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(latent_dim, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(8*8*64)(latent_inputs)\n",
    "x = layers.Reshape(target_shape=(8, 8, 64))(x)\n",
    "x = Conv2DTranspose(32, 3, strides=2, padding='same',  activation=tf.nn.relu)(x)\n",
    "# x = Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "decoder_outputs = Conv2DTranspose(3, 3, strides=2, padding='same',  activation='sigmoid')(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.mse(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='reconstruction_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 6s 9ms/step - loss: 63.8542 - reconstruction_loss: 63.6817 - kl_loss: 2.2475e-04\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 63.5577 - reconstruction_loss: 63.5125 - kl_loss: 1.1572e-04\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 63.3499 - reconstruction_loss: 63.5020 - kl_loss: 7.4846e-05\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 63.4434 - reconstruction_loss: 63.5075 - kl_loss: 5.0385e-05\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 3s 8ms/step - loss: 63.4478 - reconstruction_loss: 63.5013 - kl_loss: 4.3424e-05\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 63.6461 - reconstruction_loss: 63.5031 - kl_loss: 5.0266e-05\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 63.7728 - reconstruction_loss: 63.5024 - kl_loss: 3.5586e-05\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 63.5202 - reconstruction_loss: 63.4953 - kl_loss: 4.1226e-05\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 3s 9ms/step - loss: 63.6972 - reconstruction_loss: 63.4957 - kl_loss: 3.2220e-05\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 4s 9ms/step - loss: 63.3313 - reconstruction_loss: 63.5008 - kl_loss: 3.6204e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c900704c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "# vae.build(input_shape=(32, 32, 3))\n",
    "# vae.summary()\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "vae.fit(X_train,epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=999\n",
    "m, l, z = vae.encoder.predict(X_test[img].reshape(1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = vae.decoder.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(4, 4))\n",
    "axs = axs.flatten()\n",
    "axs[0].imshow(X_test[img])\n",
    "axs[1].imshow(dec[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
