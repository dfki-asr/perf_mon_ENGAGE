{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython import display\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import PIL\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training and testing sets are:\n",
      "X_train.shape: (50000, 32, 32, 3) & X_test.shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "\n",
    "# Load CIFAR-10 dataset-\n",
    "(train_images, y_train), (test_images, y_test) = cifar10.load_data()\n",
    "\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "# Specify hyper-parameters-\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "\n",
    "# Convert datasets to floating point types-\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "\n",
    "# Normalize the training and testing datasets-\n",
    "train_images /= 255.0\n",
    "test_images /= 255.0\n",
    "\n",
    "print(\"\\nDimensions of training and testing sets are:\")\n",
    "print(f\"X_train.shape: {train_images.shape} & X_test.shape: {test_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/root/.keras/datasets/'\n",
    "latent_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6bb51f9820>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(train_images[0,:,:], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    images = images.reshape((images.shape[0], 32, 32, 3)) / 255.\n",
    "    return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
    "\n",
    "# train_images = preprocess_images(train_images)\n",
    "# test_images = preprocess_images(test_images)\n",
    "\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 32\n",
    "test_size = test_images.shape[0]\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n",
    "                 .shuffle(train_size).batch(batch_size))\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
    "                .shuffle(test_size).batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = preprocess_images(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.23137255, 0.24313726, 0.24705882],\n",
       "        [0.16862746, 0.18039216, 0.1764706 ],\n",
       "        [0.19607843, 0.1882353 , 0.16862746],\n",
       "        ...,\n",
       "        [0.61960787, 0.5176471 , 0.42352942],\n",
       "        [0.59607846, 0.49019608, 0.4       ],\n",
       "        [0.5803922 , 0.4862745 , 0.40392157]],\n",
       "\n",
       "       [[0.0627451 , 0.07843138, 0.07843138],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.07058824, 0.03137255, 0.        ],\n",
       "        ...,\n",
       "        [0.48235294, 0.34509805, 0.21568628],\n",
       "        [0.46666667, 0.3254902 , 0.19607843],\n",
       "        [0.47843137, 0.34117648, 0.22352941]],\n",
       "\n",
       "       [[0.09803922, 0.09411765, 0.08235294],\n",
       "        [0.0627451 , 0.02745098, 0.        ],\n",
       "        [0.19215687, 0.10588235, 0.03137255],\n",
       "        ...,\n",
       "        [0.4627451 , 0.32941177, 0.19607843],\n",
       "        [0.47058824, 0.32941177, 0.19607843],\n",
       "        [0.42745098, 0.28627452, 0.16470589]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.8156863 , 0.6666667 , 0.3764706 ],\n",
       "        [0.7882353 , 0.6       , 0.13333334],\n",
       "        [0.7764706 , 0.6313726 , 0.10196079],\n",
       "        ...,\n",
       "        [0.627451  , 0.52156866, 0.27450982],\n",
       "        [0.21960784, 0.12156863, 0.02745098],\n",
       "        [0.20784314, 0.13333334, 0.07843138]],\n",
       "\n",
       "       [[0.7058824 , 0.54509807, 0.3764706 ],\n",
       "        [0.6784314 , 0.48235294, 0.16470589],\n",
       "        [0.7294118 , 0.5647059 , 0.11764706],\n",
       "        ...,\n",
       "        [0.72156864, 0.5803922 , 0.36862746],\n",
       "        [0.38039216, 0.24313726, 0.13333334],\n",
       "        [0.3254902 , 0.20784314, 0.13333334]],\n",
       "\n",
       "       [[0.69411767, 0.5647059 , 0.45490196],\n",
       "        [0.65882355, 0.5058824 , 0.36862746],\n",
       "        [0.7019608 , 0.5568628 , 0.34117648],\n",
       "        ...,\n",
       "        [0.84705883, 0.72156864, 0.54901963],\n",
       "        [0.5921569 , 0.4627451 , 0.32941177],\n",
       "        [0.48235294, 0.36078432, 0.28235295]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CVAE(tf.keras.Model):\n",
    "    \"\"\"Convolutional variational autoencoder.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "              [\n",
    "                  tf.keras.layers.InputLayer(input_shape=(32, 32, 3)),\n",
    "                  tf.keras.layers.Conv2D(\n",
    "                      filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                  tf.keras.layers.Conv2D(\n",
    "                      filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                  tf.keras.layers.Flatten(),\n",
    "                  # No activation\n",
    "                  tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "              ]\n",
    "        )\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(units=8*8*32, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Reshape(target_shape=(8, 8, 32)),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                    activation='relu'),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                    activation='relu'),\n",
    "            # No activation\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=3, kernel_size=3, strides=1, padding='same'),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    @tf.function\n",
    "    def sample(self, z=None):\n",
    "        if z is None:\n",
    "            z = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(z, apply_sigmoid=True)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 15, 15, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 12548     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,940\n",
      "Trainable params: 31,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 2048)              6144      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 16, 16, 64)       18496     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 32, 32, 3)        867       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,971\n",
      "Trainable params: 43,971\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CVAE(latent_dim)\n",
    "model.encoder.summary()\n",
    "model.decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "    \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "    This function computes the loss and gradients, and uses the latter to\n",
    "    update the model's parameters.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_images(model, n, epoch, im_size=32, save=True, first_epoch=False, f_ep_count=0):\n",
    "    \n",
    "\n",
    "    # Create image matrix \n",
    "    image_width = im_size*n\n",
    "    image_height = image_width\n",
    "    image = np.zeros((image_height, image_width))\n",
    "\n",
    "    \n",
    "\n",
    "    # Create list of values which are evenly spaced wrt probability mass\n",
    "\n",
    "    norm = tfp.distributions.Normal(0, 1)\n",
    "    grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "\n",
    "    # For each point on the grid in the latent space, decode and\n",
    "\n",
    "    # copy the image into the image array\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z = np.array([[xi, yi]])\n",
    "            x_decoded = model.sample(z)\n",
    "            digit = tf.reshape(x_decoded[0], (im_size, im_size))\n",
    "            image[i * im_size: (i + 1) * im_size,\n",
    "                  j * im_size: (j + 1) * im_size] = digit.numpy()\n",
    "    \n",
    "\n",
    "    # Plot the image array\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap='Greys_r')\n",
    "    plt.axis('Off')\n",
    "\n",
    "\n",
    "    # Potentially save, with different formatting if within first epoch\n",
    "    if save and first_epoch:\n",
    "        plt.savefig('tf_grid_at_epoch_{:04d}.{:04d}.png'.format(epoch, f_ep_count))\n",
    "    elif save:\n",
    "        plt.savefig('tf_grid_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 3072 values, but the requested shape has 1024 [Op:Reshape]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrun_functions_eagerly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplot_latent_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mplot_latent_images\u001b[0;34m(model, n, epoch, im_size, save, first_epoch, f_ep_count)\u001b[0m\n\u001b[1;32m     23\u001b[0m         z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[xi, yi]])\n\u001b[1;32m     24\u001b[0m         x_decoded \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msample(z)\n\u001b[0;32m---> 25\u001b[0m         digit \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_decoded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m         image[i \u001b[38;5;241m*\u001b[39m im_size: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m im_size,\n\u001b[1;32m     27\u001b[0m               j \u001b[38;5;241m*\u001b[39m im_size: (j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m im_size] \u001b[38;5;241m=\u001b[39m digit\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Plot the image array\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 3072 values, but the requested shape has 1024 [Op:Reshape]"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "plot_latent_images(model, 20, epoch=0)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    for idx, train_x in enumerate(train_dataset):\n",
    "        train_step(model, train_x, optimizer)\n",
    "        if epoch == 1 and idx % 75 == 0:\n",
    "            plot_latent_images(model, 20, epoch=epoch, first_epoch=True, f_ep_count=idx)          \n",
    "    end_time = time.time()\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    for test_x in test_dataset:\n",
    "        loss(compute_loss(model, test_x))\n",
    "    elbo = -loss.result()\n",
    "    #display.clear_output(wait=False)\n",
    "    print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
    "        .format(epoch, elbo, end_time - start_time))\n",
    "    if epoch != 1:\n",
    "        plot_latent_images(model, 10, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " z = tf.random.normal(shape=(100, 2))\n",
    "\n",
    "plt.plot(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = model.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dec[75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
