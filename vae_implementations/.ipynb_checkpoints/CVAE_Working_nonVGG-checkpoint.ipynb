{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Conv2DTranspose\n",
    "\n",
    "\n",
    "#########################\n",
    "#        ENCODER        #\n",
    "#########################\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, latent_dim):\n",
    "    \n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.enc_block_1 = Conv2D( \n",
    "                        filters=32, \n",
    "                        kernel_size=3, \n",
    "                        strides=(2, 2), \n",
    "                        padding = 'same',\n",
    "                        kernel_initializer=he_normal())\n",
    "    \n",
    "    \n",
    "    self.enc_block_2 = Conv2D( \n",
    "                  filters=64, \n",
    "                  kernel_size=3, \n",
    "                  strides=(2, 2), \n",
    "                  padding = 'same',\n",
    "                  kernel_initializer=he_normal())\n",
    "    \n",
    "    \n",
    "    \n",
    "    self.enc_block_3 = Conv2D( \n",
    "                  filters=128, \n",
    "                  kernel_size=3, \n",
    "                  strides=(2, 2), \n",
    "                  padding = 'same',\n",
    "                  kernel_initializer=he_normal())\n",
    "            \n",
    "\n",
    "    \n",
    "    self.enc_block_4 = Conv2D( \n",
    "                  filters=256, \n",
    "                  kernel_size=3, \n",
    "                  strides=(2, 2), \n",
    "                  padding = 'same',\n",
    "                  kernel_initializer=he_normal())\n",
    "    \n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "    self.dense = tf.keras.layers.Dense(latent_dim + latent_dim)  \n",
    "\n",
    "\n",
    "  def __call__(self, conditional_input, latent_dim, is_train):\n",
    "     # Encoder block 1\n",
    "    x = self.enc_block_1(conditional_input)\n",
    "    x = BatchNormalization(trainable = is_train)(x)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    # Encoder block 2\n",
    "    x = self.enc_block_2(x)\n",
    "    x = BatchNormalization(trainable = is_train)(x)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    # Encoder block 3\n",
    "    x = self.enc_block_3(x)\n",
    "    x = BatchNormalization(trainable = is_train)(x)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    # Encoder block 4\n",
    "    x = self.enc_block_4(x)\n",
    "    x = BatchNormalization(trainable = is_train)(x)\n",
    "    x = tf.nn.leaky_relu(x)   \n",
    "\n",
    "    x = self.dense(self.flatten(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "#        DECODER        #\n",
    "#########################\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "\n",
    "  def __init__(self, batch_size = 32):\n",
    "\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.batch_size = batch_size\n",
    "    self.dense = tf.keras.layers.Dense(4*4*self.batch_size*8)\n",
    "    self.reshape = tf.keras.layers.Reshape(target_shape=(4, 4, self.batch_size*8))\n",
    "\n",
    "    self.dec_block_1 = Conv2DTranspose(\n",
    "            filters=256,\n",
    "            kernel_size=3,\n",
    "            strides=(2, 2),\n",
    "            padding='same',\n",
    "            kernel_initializer=he_normal())\n",
    "\n",
    "    self.dec_block_2 = Conv2DTranspose(\n",
    "            filters=128,\n",
    "            kernel_size=3,\n",
    "            strides=(2, 2),\n",
    "            padding='same',\n",
    "            kernel_initializer=he_normal())\n",
    "        \n",
    "    self.dec_block_3 = Conv2DTranspose(\n",
    "            filters=64,\n",
    "            kernel_size=3,\n",
    "            strides=(2, 2),\n",
    "            padding='same',\n",
    "            kernel_initializer=he_normal())\n",
    "\n",
    "    self.dec_block_4 = Conv2DTranspose(\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            strides=(2, 2),\n",
    "            padding='same',\n",
    "            kernel_initializer=he_normal())\n",
    "\n",
    "    self.dec_block_5 = Conv2DTranspose(\n",
    "            filters=3, \n",
    "            kernel_size=3, \n",
    "            strides=(1, 1), \n",
    "            padding='same',\n",
    "            kernel_initializer=he_normal())\n",
    "\n",
    "  def __call__(self, z_cond, is_train):\n",
    "    # Reshape input\n",
    "    x = self.dense(z_cond)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    x = self.reshape(x)\n",
    "    # Decoder block 1\n",
    "    x = self.dec_block_1(x)\n",
    "    x = BatchNormalization(trainable = is_train)(x)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    # Decoder block 2\n",
    "    x = self.dec_block_2(x)\n",
    "    x = BatchNormalization(trainable = is_train)(x)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    # Decoder block 3\n",
    "    x = self.dec_block_3(x)\n",
    "    x = BatchNormalization(trainable = is_train)(x)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    # Decoder block 4\n",
    "    x = self.dec_block_4(x)\n",
    "    x = BatchNormalization(trainable = is_train)(x)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "\n",
    "    return self.dec_block_5(x)\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "#       Conv-CVAE       #\n",
    "#########################\n",
    "\n",
    "class ConvCVAE (tf.keras.Model) :\n",
    "\n",
    "    def __init__(self, \n",
    "        encoder,\n",
    "        decoder,\n",
    "        label_dim,\n",
    "        latent_dim,\n",
    "        batch_size = 32,\n",
    "        beta = 1,\n",
    "        image_dim = [64, 64, 3]):\n",
    "\n",
    "        super(ConvCVAE, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.label_dim = label_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.beta = beta = 1\n",
    "        self.image_dim = image_dim = [64, 64, 3]              \n",
    "\n",
    "\n",
    "    def __call__(self, inputs, is_train):\n",
    "    \n",
    "        input_img, input_label, conditional_input = self.conditional_input(inputs)\n",
    "\n",
    "        z_mean, z_log_var = tf.split(self.encoder(conditional_input, self.latent_dim, is_train), num_or_size_splits=2, axis=1)    \n",
    "        z_cond = self.reparametrization(z_mean, z_log_var, input_label)\n",
    "        logits = self.decoder(z_cond, is_train)\n",
    "\n",
    "        recon_img = tf.nn.sigmoid(logits)\n",
    "\n",
    "        # Loss computation #\n",
    "        latent_loss = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1) # KL divergence\n",
    "        reconstr_loss = np.prod((64,64)) * tf.keras.losses.binary_crossentropy(tf.keras.backend.flatten(input_img), tf.keras.backend.flatten(recon_img)) # over weighted MSE\n",
    "        loss = reconstr_loss + self.beta * latent_loss # weighted ELBO loss\n",
    "        loss = tf.reduce_mean(loss) \n",
    "\n",
    "        return {\n",
    "                    'recon_img': recon_img,\n",
    "                    'latent_loss': latent_loss,\n",
    "                    'reconstr_loss': reconstr_loss,\n",
    "                    'loss': loss,\n",
    "                    'z_mean': z_mean,\n",
    "                    'z_log_var': z_log_var\n",
    "                }\n",
    "\n",
    "\n",
    "    def conditional_input(self, inputs):\n",
    "        \"\"\" Builds the conditional input and returns the original input images, their labels and the conditional input.\"\"\"\n",
    "\n",
    "        input_img = tf.keras.layers.InputLayer(input_shape=self.image_dim, dtype = 'float32')(inputs[0])\n",
    "        input_label = tf.keras.layers.InputLayer(input_shape=(self.label_dim,), dtype = 'float32')(inputs[1])\n",
    "        labels = tf.reshape(inputs[1], [-1, 1, 1, self.label_dim]) #batch_size, 1, 1, label_size\n",
    "        ones = tf.ones([inputs[0].shape[0]] + self.image_dim[0:-1] + [self.label_dim]) #batch_size, 64, 64, label_size\n",
    "        labels = ones * labels #batch_size, 64, 64, label_size\n",
    "        conditional_input = tf.keras.layers.InputLayer(input_shape=(self.image_dim[0], self.image_dim[1], self.image_dim[2] + self.label_dim), dtype = 'float32')(tf.concat([inputs[0], labels], axis=3))\n",
    "\n",
    "        return input_img, input_label, conditional_input\n",
    "\n",
    "\n",
    "    def reparametrization(self, z_mean, z_log_var, input_label):\n",
    "        \"\"\" Performs the riparametrization trick\"\"\"\n",
    "\n",
    "        eps = tf.random.normal(shape = (input_label.shape[0], self.latent_dim), mean = 0.0, stddev = 1.0)       \n",
    "        z = z_mean + tf.math.exp(z_log_var * .5) * eps\n",
    "        z_cond = tf.concat([z, input_label], axis=1) # (batch_size, label_dim + latent_dim)\n",
    "\n",
    "        return z_cond\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "# Train Step Function #\n",
    "#######################\n",
    "\n",
    "\n",
    "def train_step(data, model, optimizer):\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        model_output = model(data, is_train = True)\n",
    "\n",
    "    trainable_variables = model.trainable_variables\n",
    "    grads = tape.gradient(model_output['loss'], trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "    total_loss = model_output['loss'].numpy().mean()\n",
    "    recon_loss = model_output['reconstr_loss'].numpy().mean()\n",
    "    latent_loss = model_output['latent_loss'].numpy().mean()\n",
    "\n",
    "    return total_loss, recon_loss, latent_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "# Encoding and Decoding methods  #\n",
    "##################################\n",
    "\n",
    "\n",
    "def encode(self, inputs, label):\n",
    "    \"\"\" Encodes the input into the latent space.\"\"\"\n",
    "    return self.sess.run(self.z_mean, feed_dict={self.x: inputs, self.y: label})\n",
    "\n",
    "\n",
    "def decode(self, label, z = None):\n",
    "    \"\"\" \n",
    "    Generates data starting from the point z in the latent space.\n",
    "    If z is None, z is drawn from prior in latent space.\n",
    "    \"\"\"\n",
    "    if z is None:\n",
    "        z = 0.0 + np.random.randn(self.batch_size, self.latent_dim) * 0.75\n",
    "    return self.sess.run(self.generated_image, feed_dict={self.z_sample_3: z, self.y: label})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "#  Utils for plotting  #\n",
    "########################\n",
    "\n",
    "\n",
    "def batch_generator(batch_dim, test_labels, model_name):\n",
    "    \"\"\"\n",
    "    Batch generator using the given list of labels.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_imgs = []\n",
    "        labels = []\n",
    "        for label in (test_labels):\n",
    "            labels.append(label)\n",
    "            if len(labels) == batch_dim:\n",
    "                batch_imgs = create_image_batch(labels, model_name)\n",
    "                batch_labels = [x[1] for x in labels]\n",
    "                yield np.asarray(batch_imgs), np.asarray(batch_labels)\n",
    "                batch_imgs = []\n",
    "                labels = []\n",
    "                batch_labels = []\n",
    "        if batch_imgs:\n",
    "            yield np.asarray(batch_imgs), np.asarray(batch_labels)\n",
    "\n",
    "\n",
    "def get_image(image_path, model_name, img_size = 128, img_resize = 64, x = 25, y = 45):\n",
    "    \"\"\"\n",
    "    Crops, resizes and normalizes the target image.\n",
    "        - If model_name == Dense, the image is returned as a flattened numpy array with dim (64*64*3)\n",
    "        - otherwise, the image is returned as a numpy array with dim (64,64,3)\n",
    "    \"\"\"\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img = img[y:y+img_size, x:x+img_size]\n",
    "    img = cv2.resize(img, (img_resize, img_resize))\n",
    "    img = np.array(img, dtype='float32')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img /= 255.0 # Normalization to [0.,1.]\n",
    "\n",
    "    if model_name == \"Dense\" :\n",
    "        img = img.ravel()\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def create_image_batch(labels, model_name):\n",
    "    \"\"\"\n",
    "    Returns the list of images corresponding to the given labels.\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    imgs_id = [item[0] for item in labels]\n",
    "\n",
    "    for i in imgs_id:\n",
    "        image_path ='/input/CelebA/img_align_celeba/img_align_celeba/' + i\n",
    "        imgs.append(get_image(image_path, model_name))\n",
    "\n",
    "    return imgs\n",
    "\n",
    "\n",
    "\n",
    "def convert_batch_to_image_grid(image_batch, dim = 64):\n",
    "    reshaped = (image_batch.reshape(4, 8, dim, dim, 3)\n",
    "              .transpose(0, 2, 1, 3, 4)\n",
    "              .reshape(4 * dim, 8 * dim, 3))\n",
    "    return reshaped \n",
    "\n",
    "\n",
    "def imshow_grid(imgs, model_name, shape=[2, 5], name='default', save=False):\n",
    "    \"\"\"Plot images in a grid of a given shape.\"\"\"\n",
    "    fig = plt.figure(1)\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=shape, axes_pad=0.05)\n",
    "    size = shape[0] * shape[1]\n",
    "    if model_name == \"Dense\":\n",
    "        for i in range(size):\n",
    "            grid[i].axis('off')\n",
    "            grid[i].imshow(imgs[i].reshape(64, 64, 3))  \n",
    "        if save:\n",
    "            plt.savefig(str(name) + '.png')\n",
    "            plt.clf()\n",
    "        else:\n",
    "            plt.show()\n",
    "    else:\n",
    "        for i in range(size):\n",
    "            grid[i].axis('off')\n",
    "            grid[i].imshow(imgs[i])  \n",
    "        if save:\n",
    "            plt.savefig(str(name) + '.png')\n",
    "            plt.clf()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "##########################################\n",
    "#   Utils to save and read pickle files  #\n",
    "##########################################\n",
    "\n",
    "def save_data(file_name, data):\n",
    "    \"\"\"\n",
    "    Saves data on file_name.pickle.\n",
    "    \"\"\"\n",
    "    with open((file_name+'.pickle'), 'wb') as openfile:\n",
    "        print(type(data))\n",
    "        pickle.dump(data, openfile)\n",
    "\n",
    "\n",
    "def read_data(file_name):\n",
    "    \"\"\"\n",
    "    Reads file_name.pickle and returns its content.\n",
    "    \"\"\"\n",
    "    with (open((file_name+'.pickle'), \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects=pickle.load(openfile)\n",
    "            except EOFError:\n",
    "                break\n",
    "    return objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
