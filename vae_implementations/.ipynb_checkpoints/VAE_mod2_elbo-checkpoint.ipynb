{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Input, Flatten,\\\n",
    "Conv2DTranspose, BatchNormalization, LeakyReLU, Reshape, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import plotly\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "img_chn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset-\n",
    "(train_images, y_train), (test_images, y_test) = cifar10.load_data()\n",
    "if img_chn == 1:\n",
    "    train_images = train_images.mean(axis=3)                                                                                    \n",
    "    test_images = test_images.mean(axis=3)                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_rows, img_cols, img_chn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training and testing sets are:\n",
      "X_train.shape: (50000, 32, 32, 1) & X_test.shape: (10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Specify hyper-parameters-\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "\n",
    "# Convert datasets to floating point types-\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "\n",
    "# Normalize the training and testing datasets-\n",
    "if img_chn == 1:\n",
    "    train_images = train_images.reshape((train_images.shape[0], \\\n",
    "                                         img_rows, img_rows, img_chn)) / 255.\n",
    "    test_images = test_images.reshape((test_images.shape[0], \\\n",
    "                                      img_rows, img_rows, img_chn)) / 255.\n",
    "else:\n",
    "    train_images = train_images/255.\n",
    "    test_images = test_images/255.\n",
    "\n",
    "\n",
    "print(\"\\nDimensions of training and testing sets are:\")\n",
    "print(f\"X_train.shape: {train_images.shape} & X_test.shape: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_creation(X_data, batch_size):\n",
    "    end = X_data.shape[0]-1\n",
    "    start = 0\n",
    "    batch_split_X = list()\n",
    "    while start < end:\n",
    "        img_slice = np.array(X_data[start:start+batch_size])\n",
    "        batch_split_X.append(img_slice)\n",
    "        start = start + batch_size\n",
    "    return np.array(batch_split_X, dtype=object)\n",
    "    \n",
    "train_dataset = batch_creation(train_images, 32)\n",
    "test_dataset = batch_creation(test_images, 32)\n",
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_outlier_detector = True\n",
    "\n",
    "latent_dim = 1024\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "# train_size = train_images.shape[0]\n",
    "# batch_size = 32\n",
    "# test_size = test_images.shape[0]\n",
    "\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "# train_dataset = train_dataset.shuffle(train_size)\n",
    "# train_dataset = train_dataset.batch(batch_size)\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices(test_images)\\\n",
    "#             .shuffle(test_size).batch(batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    \"\"\"Convolutional variational autoencoder.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "              [\n",
    "                  tf.keras.layers.InputLayer(input_shape),\n",
    "                  tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Conv2D(64, 3, strides=2,  padding='same', activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Conv2D(128, 3, strides=2,  padding='same', activation=tf.nn.relu),\n",
    "                  tf.keras.layers.Flatten(),\n",
    "                  # No activation\n",
    "                  tf.keras.layers.Dense(latent_dim + latent_dim ),\n",
    "              ]\n",
    "        )\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(units=4*4*128, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Reshape(target_shape=(4, 4, 128)),\n",
    "            tf.keras.layers.Conv2DTranspose(64, 3, strides=2, padding='same',  activation=tf.nn.relu),\n",
    "            tf.keras.layers.Conv2DTranspose(32, 3, strides=2, padding='same',  activation=tf.nn.relu),\n",
    "            # No activation\n",
    "            tf.keras.layers.Conv2DTranspose(img_chn, 3, strides=2, padding='same'),\n",
    "#             No activation\n",
    "#             tf.keras.layers.Conv2DTranspose(\n",
    "#                 filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    @tf.function\n",
    "    def sample(self, z=None):\n",
    "        if z is None:\n",
    "            z = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(z, apply_sigmoid=True)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "    @tf.function\n",
    "    def train_step(self, x, optimizer):\n",
    "        \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "        This function computes the loss and gradients, and uses the latter to\n",
    "        update the model's parameters.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = compute_loss(model, x)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 32)        320       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,289,024\n",
      "Trainable params: 4,289,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 2048)              2099200   \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 8, 8, 64)         73792     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  (None, 16, 16, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 32, 32, 1)        289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,191,745\n",
      "Trainable params: 2,191,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CVAE(latent_dim)\n",
    "model.encoder.summary()\n",
    "model.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_images(model, n, epoch, im_size=32, save=True, first_epoch=False, f_ep_count=0):\n",
    "    pass\n",
    "    \n",
    "#     # Create image matrix \n",
    "#     image_width = im_size*n\n",
    "#     image_height = image_width\n",
    "#     image = np.zeros((image_height, image_width, img_chn))\n",
    "\n",
    "#     # Create list of values which are evenly spaced wrt probability mass\n",
    "\n",
    "#     norm = tfp.distributions.Normal(0, 1)\n",
    "#     grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "#     grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    \n",
    "#     # For each point on the grid in the latent space, decode and\n",
    "\n",
    "# #     # copy the image into the image array\n",
    "# #     for i, yi in enumerate(grid_x):\n",
    "# #         for j, xi in enumerate(grid_y):\n",
    "# #             z = np.array([[xi, yi]])\n",
    "# #             x_decoded = model.sample(z)\n",
    "# #             digit = tf.reshape(x_decoded[0], (im_size, im_size, img_chn))\n",
    "# #             image[i * im_size: (i + 1) * im_size,\n",
    "# #                   j * im_size: (j + 1) * im_size] = digit.numpy()\n",
    "    \n",
    "\n",
    "#     # Plot the image array\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.imshow(image, cmap='Greys_r')\n",
    "#     plt.axis('Off')\n",
    "\n",
    "\n",
    "#     # Potentially save, with different formatting if within first epoch\n",
    "#     if save and first_epoch:\n",
    "#         plt.savefig('tf_grid_at_epoch_{:04d}.{:04d}.png'.format(epoch, f_ep_count))\n",
    "#     elif save:\n",
    "#         plt.savefig('tf_grid_at_epoch_{:04d}.png'.format(epoch))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Test set ELBO: -709.6585693359375, time elapse for current epoch: 10.08524465560913\n",
      "Epoch: 2, Test set ELBO: -709.206298828125, time elapse for current epoch: 8.434650897979736\n",
      "Epoch: 3, Test set ELBO: -692.9951782226562, time elapse for current epoch: 8.975142478942871\n",
      "Epoch: 4, Test set ELBO: -691.05712890625, time elapse for current epoch: 8.013568639755249\n",
      "Epoch: 5, Test set ELBO: -690.3988647460938, time elapse for current epoch: 8.879610061645508\n",
      "Epoch: 6, Test set ELBO: -690.5565795898438, time elapse for current epoch: 8.720054626464844\n",
      "Epoch: 7, Test set ELBO: -689.882568359375, time elapse for current epoch: 8.935361862182617\n",
      "Epoch: 8, Test set ELBO: -689.666748046875, time elapse for current epoch: 8.90344500541687\n",
      "Epoch: 9, Test set ELBO: -689.5504760742188, time elapse for current epoch: 8.64024806022644\n",
      "Epoch: 10, Test set ELBO: -689.5183715820312, time elapse for current epoch: 8.753756999969482\n",
      "Epoch: 11, Test set ELBO: -689.2171020507812, time elapse for current epoch: 8.505372047424316\n",
      "Epoch: 12, Test set ELBO: -689.1931762695312, time elapse for current epoch: 8.944044351577759\n",
      "Epoch: 13, Test set ELBO: -689.2681274414062, time elapse for current epoch: 8.879309892654419\n",
      "Epoch: 14, Test set ELBO: -689.1679077148438, time elapse for current epoch: 8.781915664672852\n",
      "Epoch: 15, Test set ELBO: -689.182861328125, time elapse for current epoch: 9.323206901550293\n",
      "Epoch: 16, Test set ELBO: -689.1100463867188, time elapse for current epoch: 8.97172212600708\n",
      "Epoch: 17, Test set ELBO: -689.1262817382812, time elapse for current epoch: 8.723770141601562\n",
      "Epoch: 18, Test set ELBO: -688.9668579101562, time elapse for current epoch: 8.972081899642944\n",
      "Epoch: 19, Test set ELBO: -689.0650634765625, time elapse for current epoch: 8.315105199813843\n",
      "Epoch: 20, Test set ELBO: -688.9784545898438, time elapse for current epoch: 8.563543319702148\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tf.config.run_functions_eagerly(False)\n",
    "plot_latent_images(model, 10, epoch=0)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    for idx, train_x in enumerate(train_dataset):\n",
    "        model.train_step(train_x, optimizer)\n",
    "        if epoch == 1 and idx % 75 == 0:\n",
    "            plot_latent_images(model, 10, epoch=epoch, first_epoch=True, f_ep_count=idx)          \n",
    "    end_time = time.time()\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    for test_x in test_dataset:\n",
    "        loss(compute_loss(model, test_x))\n",
    "    elbo = -loss.result()\n",
    "    #display.clear_output(wait=False)\n",
    "    print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
    "        .format(epoch, elbo, end_time - start_time))\n",
    "    if epoch != 1:\n",
    "        plot_latent_images(model, 20, epoch=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, var = model.encode(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.reparameterize(mean, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('tsne_1024_mod2_elbo.npz', var1=tsne_results, var2=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(tsne_results[y_test==1,0], tsne_results[y_test==1,1], tsne_results[y_test==1,2], label=1)\n",
    "ax.scatter(tsne_results[y_test==2,0], tsne_results[y_test==2,1], tsne_results[y_test==2,2], label=2)\n",
    "\n",
    "np.savez('tsne_1024_mod2_elbo.npz', var1=tsne_results, var2=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_flat = test_images.reshape(10000, 32*32*1)\n",
    "tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results_img = tsne.fit_transform(test_img_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "for i in range(10):\n",
    "    ax.scatter(tsne_results_img[y_test==i,0], tsne_results_img[y_test==i,1], tsne_results_img[y_test==i,2], label=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon = model.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd62dda0430>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "i=5\n",
    "plt.imshow(test_images[i] , cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd62dd09190>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(recon[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
