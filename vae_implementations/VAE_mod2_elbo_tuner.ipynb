{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Input, Flatten,\\\n",
    "Conv2DTranspose, BatchNormalization, LeakyReLU, Reshape, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.datasets import cifar10, mnist\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner\n",
    "import time\n",
    "# import plotly\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training and testing sets are:\n",
      "X_train.shape: (50000, 32, 32, 1) & X_test.shape: (10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "img_chn = 1\n",
    "\n",
    "# Load CIFAR-10 dataset-\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "if img_chn == 1:\n",
    "    x_train = x_train.mean(axis=3)                                                                                    \n",
    "    x_test = x_test.mean(axis=3)           \n",
    "    \n",
    "input_shape = (img_rows, img_cols, img_chn)\n",
    "\n",
    "# Convert datasets to floating point types-\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize the training and testing datasets-\n",
    "if img_chn == 1:\n",
    "    x_train = x_train.reshape((x_train.shape[0], \\\n",
    "                                         img_rows, img_rows, img_chn)) / 255.\n",
    "    x_test = x_test.reshape((x_test.shape[0], \\\n",
    "                                      img_rows, img_rows, img_chn)) / 255.\n",
    "else:\n",
    "    x_train = x_train/255.\n",
    "    x_test = x_test/255.\n",
    "\n",
    "\n",
    "print(\"\\nDimensions of training and testing sets are:\")\n",
    "print(f\"X_train.shape: {x_train.shape} & X_test.shape: {x_test.shape}\")\n",
    "\n",
    "load_outlier_detector = True\n",
    "\n",
    "latent_dim = 2\n",
    "epochs =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_creation(X_data, batch_size):\n",
    "    end = X_data.shape[0]-1\n",
    "    start = 0\n",
    "    batch_split_X = list()\n",
    "    while start < end:\n",
    "        img_slice = np.array(X_data[start:start+batch_size])\n",
    "        batch_split_X.append(img_slice)\n",
    "        start = start + batch_size\n",
    "    return np.array(batch_split_X, dtype=object)\n",
    "    \n",
    "train_dataset = batch_creation(x_train, 32)\n",
    "test_dataset = batch_creation(x_test, 32)\n",
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_images(model, n, epoch, im_size=32, save=True, first_epoch=False, f_ep_count=0):\n",
    "    \n",
    "    # Create image matrix \n",
    "    image_width = im_size*n\n",
    "    image_height = image_width\n",
    "    image = np.zeros((image_height, image_width, img_chn))\n",
    "\n",
    "    # Create list of values which are evenly spaced wrt probability mass\n",
    "\n",
    "    norm = tfp.distributions.Normal(0, 1)\n",
    "    grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    \n",
    "    # For each point on the grid in the latent space, decode and\n",
    "\n",
    "    # copy the image into the image array\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z = np.array([[xi, yi]])\n",
    "            x_decoded = model.sample(z)\n",
    "            digit = tf.reshape(x_decoded[0], (im_size, im_size, img_chn))\n",
    "            image[i * im_size: (i + 1) * im_size,\n",
    "                  j * im_size: (j + 1) * im_size] = digit.numpy()\n",
    "    \n",
    "\n",
    "    # Plot the image array\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap='Greys_r')\n",
    "    plt.axis('Off')\n",
    "\n",
    "\n",
    "    # Potentially save, with different formatting if within first epoch\n",
    "#     if save and first_epoch:\n",
    "#         plt.savefig('tf_grid_at_epoch_{:04d}.{:04d}.png'.format(epoch, f_ep_count))\n",
    "#     elif save:\n",
    "#         plt.savefig('tf_grid_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar, z = model.encode(x)\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "latent_dim = 2\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "img_chn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,hp):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n",
    "        # adding filter size or kernel size\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\\\n",
    "        #activation function\n",
    "        activation='relu',input_shape=(32,32,1))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=hp.Int('conv_2_filter', min_value=32, max_value=128, step=16),\n",
    "        # adding filter size or kernel size\n",
    "        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\\\n",
    "        #activation function\n",
    "        activation='relu')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters=hp.Int('conv_3_filter', min_value=32, max_value=128, step=16),\n",
    "        # adding filter size or kernel size\n",
    "        kernel_size=hp.Choice('conv_3_kernel', values = [3,5]),\\\n",
    "        #activation function\n",
    "        activation='relu')\n",
    "        self.flat1 = tf.keras.layers.Flatten()\n",
    "        self.dens1 = tf.keras.layers.Dense(2)\n",
    "        self.dens2 = tf.keras.layers.Dense(2)\n",
    "        self.sampl = Sampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flat1(x)\n",
    "        mu = self.dens1(x)\n",
    "        va = self.dens2(x)\n",
    "        z = self.sampl([mu,va])\n",
    "        \n",
    "        return mu, va, z\n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(32,32,1))\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dens3 = tf.keras.layers.Dense(units=4*4*128, activation=tf.nn.relu, input_shape=(2,))\n",
    "        self.resh1 = tf.keras.layers.Reshape(target_shape=(4, 4, 128))\n",
    "        self.deco1 = tf.keras.layers.Conv2DTranspose(64, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        self.deco2 = tf.keras.layers.Conv2DTranspose(32, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        # No activation\n",
    "        self.deco3 = tf.keras.layers.Conv2DTranspose(img_chn, 3, strides=2, padding='same')\n",
    "        \n",
    "    def call(self, inp):\n",
    "        x = self.dens3(inp)\n",
    "        x = self.resh1(x)\n",
    "        x = self.deco1(x)\n",
    "        x = self.deco2(x)\n",
    "        x = self.deco3(x)\n",
    "        return x\n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(2))\n",
    "        return Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(tf.keras.Model):\n",
    "    def __init__(self,hp):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = Encoder(hp)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def call(self, inp):\n",
    "        mu, va, z = self.encoder(inp)\n",
    "        out_decoder = self.decoder(z)\n",
    "        return z, out_decoder \n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(img_rows,img_cols,img_chn))\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "    def encode(self, img):\n",
    "        return self.encoder(img)\n",
    "    \n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "    \n",
    "    @tf.function\n",
    "    def sample(self, z=None):\n",
    "        if z is None:\n",
    "            z = tf.random.normal(shape=(100, latent_dim))\n",
    "        return self.decode(z, apply_sigmoid=True)\n",
    "    \n",
    "#     @tf.function\n",
    "#     def train_step(self, x, optimizer):\n",
    "#         \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "#         This function computes the loss and gradients, and uses the latter to\n",
    "#         update the model's parameters.\n",
    "#         \"\"\"\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             loss = compute_loss(model, x)\n",
    "#         gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE_Hypermodel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        return AE(hp)\n",
    "    \n",
    "    @tf.function\n",
    "    def sample(self, z=None):\n",
    "        if z is None:\n",
    "            z = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(z, apply_sigmoid=True)\n",
    "\n",
    "    def fit(self, hp, model, x, y, validation_data, callbacks=None, **kwargs):\n",
    "        # Convert the datasets to tf.data.Dataset.\n",
    "        batch_size = hp.Int(\"batch_size\", 32, 128, step=32, default=64)\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((x, y)).batch(\n",
    "            batch_size)\n",
    "        validation_data = tf.data.Dataset.from_tensor_slices(validation_data).batch(\n",
    "            batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "#         loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        loss_fn = compute_loss\n",
    "\n",
    "        # The metric to track validation loss.\n",
    "        epoch_loss_metric = keras.metrics.Mean()\n",
    "        \n",
    "        @tf.function\n",
    "        def run_train_step(x, optimizer):\n",
    "            \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "            This function computes the loss and gradients, and uses the latter to\n",
    "            update the model's parameters.\n",
    "            \"\"\"\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = loss_fn(model, x)\n",
    "                if model.losses:\n",
    "                    loss += tf.math.add_n(model.losses) \n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "#         # Function to run the validation step.\n",
    "# #         @tf.function\n",
    "# #         def run_val_step(images, labels):\n",
    "# #             logits = model(images)\n",
    "# #             loss = loss_fn(labels, logits)\n",
    "# #             # Update the metric.\n",
    "# #             epoch_loss_metric.update_state(loss)\n",
    "\n",
    "        # Assign the model to the callbacks.\n",
    "        for callback in callbacks:\n",
    "            callback.model = model\n",
    "\n",
    "        # Record the best validation loss value\n",
    "        best_epoch_loss = float(\"inf\")\n",
    "\n",
    "        # Define the optimizer.\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\", default=1e-3)\n",
    "        )\n",
    "        \n",
    "        # The custom training and validation loop.\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            start_time = time.time()\n",
    "            for idx, train_x in enumerate(train_dataset):\n",
    "                run_train_step(train_x, optimizer)\n",
    "                if epoch == 1 and idx % 75 == 0:\n",
    "                    plot_latent_images(model, 10, epoch=epoch, first_epoch=True, f_ep_count=idx)          \n",
    "            end_time = time.time()\n",
    "            loss = tf.keras.metrics.Mean()  # chooses MEAN as the reduction strategy between epochs\n",
    "\n",
    "            for test_x in test_dataset:\n",
    "                loss(compute_loss(model, test_x))  # mean of the compute_loss\n",
    "            elbo = -loss.result()\n",
    "            #display.clear_output(wait=False)\n",
    "            print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
    "                .format(epoch, elbo, end_time - start_time))\n",
    "            if epoch != 1:\n",
    "                plot_latent_images(model, 20, epoch=epoch)\n",
    "\n",
    "            # Calling the callbacks after epoch.\n",
    "            epoch_loss = float(elbo)\n",
    "            for callback in callbacks:\n",
    "                # The \"my_metric\" is the objective passed to the tuner.\n",
    "                callback.on_epoch_end(epoch, logs={\"my_metric\": epoch_loss})\n",
    "            epoch_loss_metric.reset_states()\n",
    "\n",
    "            print(f\"Epoch loss: {epoch_loss}\")\n",
    "            best_epoch_loss = min(best_epoch_loss, epoch_loss)\n",
    "\n",
    "        # Return the evaluation metric value.\n",
    "        return best_epoch_loss\n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_model = CVAE_Hypermodel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " encoder (Encoder)           ((None, 2),               105348    \n",
      "                              (None, 2),                         \n",
      "                              (None, 2))                         \n",
      "                                                                 \n",
      " decoder (Decoder)           (None, 32, 32, 1)         98689     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 204,037\n",
      "Trainable params: 204,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "***********************************\n",
      "********************************\n",
      "))\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 30, 30, 32)   320         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 28, 28, 32)   9248        ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 26, 26, 32)   9248        ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 21632)        0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2)            43266       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            43266       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 2)            0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 105,348\n",
      "Trainable params: 105,348\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "***********************************\n",
      "********************************\n",
      "))\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2048)              6144      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 8, 8, 64)         73792     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 16, 16, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 32, 32, 1)        289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 98,689\n",
      "Trainable params: 98,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "model = h_model.build(hp)\n",
    "model.model().summary()\n",
    "print(\"***********************************\\n********************************\\n))\")\n",
    "model.encoder.model().summary()\n",
    "print(\"***********************************\\n********************************\\n))\")\n",
    "model.decoder.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    objective=keras_tuner.Objective(\"my_metric\", \"min\"),\n",
    "    max_trials=2,\n",
    "    hypermodel=CVAE_Hypermodel(),\n",
    "    directory=\"results\",\n",
    "    project_name=\"custom_training\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 06m 05s]\n",
      "my_metric: -659.1856689453125\n",
      "\n",
      "Best my_metric So Far: -659.1856689453125\n",
      "Total elapsed time: 00h 10m 56s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tuner.search(x=x_train, y=y_train, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in results/custom_training\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x7f568853af10>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv_1_filter: 48\n",
      "conv_1_kernel: 5\n",
      "conv_2_filter: 48\n",
      "conv_2_kernel: 3\n",
      "conv_3_filter: 112\n",
      "conv_3_kernel: 3\n",
      "batch_size: 64\n",
      "learning_rate: 0.00015978705252452907\n",
      "Score: -659.1856689453125\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "conv_1_filter: 112\n",
      "conv_1_kernel: 5\n",
      "conv_2_filter: 64\n",
      "conv_2_kernel: 3\n",
      "conv_3_filter: 96\n",
      "conv_3_kernel: 3\n",
      "batch_size: 64\n",
      "learning_rate: 0.001\n",
      "Score: -657.7086791992188\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv_1_filter': 48, 'conv_1_kernel': 5, 'conv_2_filter': 48, 'conv_2_kernel': 3, 'conv_3_filter': 112, 'conv_3_kernel': 3, 'batch_size': 64, 'learning_rate': 0.00015978705252452907}\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "len(tuner.get_best_hyperparameters())\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv_1_filter': 48,\n",
       " 'conv_1_kernel': 5,\n",
       " 'conv_2_filter': 48,\n",
       " 'conv_2_kernel': 3,\n",
       " 'conv_3_filter': 112,\n",
       " 'conv_3_kernel': 3,\n",
       " 'batch_size': 64,\n",
       " 'learning_rate': 0.00015978705252452907}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.build(input_shape=(None,32,32,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 28, 28, 48)   1248        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 26, 26, 48)   20784       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 24, 24, 112)  48496       ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64512)        0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2)            129026      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            129026      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 2)            0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 328,580\n",
      "Trainable params: 328,580\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.encoder.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f55f040c130>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "i=8\n",
    "plt.imshow(x_train[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = best_model.encode(x_train[i].reshape(1,32,32,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f558f3b7f40>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "d = best_model.decode(c)\n",
    "plt.imshow(d[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
