{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Input, Flatten,\\\n",
    "Conv2DTranspose, BatchNormalization, LeakyReLU, Reshape, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner\n",
    "import time\n",
    "# import plotly\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "latent_dim = 2\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "img_chn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VAE\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 16, 16, 32)   320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 8, 8, 64)     18496       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 4, 4, 128)    73856       ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2)            4098        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            4098        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " sampling_layer (Sampling)      (None, 2)            0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2048)         6144        ['sampling_layer[0][0]']         \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 4, 4, 128)    0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 8, 8, 64)    73792       ['reshape[0][0]']                \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 16, 16, 32)  18464       ['conv2d_transpose[0][0]']       \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 32, 32, 1)   289         ['conv2d_transpose_1[0][0]']     \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 199,557\n",
      "Trainable params: 199,557\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = keras.Input(shape=(32, 32, 1))\n",
    "#first conv\n",
    "x =  Conv2D(32, 3, strides=2, padding='same', activation=tf.nn.relu)(encoder_inputs)\n",
    "x =  Conv2D(64, 3, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "x =  Conv2D(128, 3, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "x = Flatten()(x)\n",
    "z_mean = Dense(latent_dim )(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "z = Sampling(name='sampling_layer')([z_mean, z_log_var])\n",
    "\n",
    "# self.encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "#         encoder.summary()\n",
    "\n",
    "# latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = Dense(units=4*4*128, activation=tf.nn.relu)(z)\n",
    "x = Reshape(target_shape=(4, 4, 128))(x)\n",
    "x = Conv2DTranspose(64, 3, strides=2, padding='same',  activation=tf.nn.relu)(x)\n",
    "x = Conv2DTranspose(32, 3, strides=2, padding='same',  activation=tf.nn.relu)(x)\n",
    "dec_out = Conv2DTranspose(img_chn, 3, strides=2, padding='same')(x)\n",
    "\n",
    "\n",
    "#         decoder.summary()\n",
    "vae_model = Model(encoder_inputs, dec_out, name=\"VAE\")\n",
    "vae_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.flat1 = tf.keras.layers.Flatten()\n",
    "        self.dens1 = tf.keras.layers.Dense(2)\n",
    "        self.dens2 = tf.keras.layers.Dense(2)\n",
    "        self.sampl = Sampling()\n",
    "        self.decIn = tf.keras.layers.InputLayer(input_shape=(2,))\n",
    "        self.dens3 = tf.keras.layers.Dense(units=4*4*128, activation=tf.nn.relu)\n",
    "        self.resh1 = tf.keras.layers.Reshape(target_shape=(4, 4, 128))\n",
    "        self.deco1 = tf.keras.layers.Conv2DTranspose(64, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        self.deco2 = tf.keras.layers.Conv2DTranspose(32, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        # No activation\n",
    "        self.deco3 = tf.keras.layers.Conv2DTranspose(img_chn, 3, strides=2, padding='same')\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flat1(x)\n",
    "        mu = self.dens1(x)\n",
    "        va = self.dens2(x)\n",
    "        z = self.sampl([mu,va])\n",
    "        x = self.decIn(z)\n",
    "        x = self.dens3(x)\n",
    "        x = self.resh1(x)\n",
    "        x = self.deco1(x)\n",
    "        x = self.deco2(x)\n",
    "        x = self.deco3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.build(input_shape=(None,32,32,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation=tf.nn.relu, input_shape=(32,32,1))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.flat1 = tf.keras.layers.Flatten()\n",
    "        self.dens1 = tf.keras.layers.Dense(2)\n",
    "        self.dens2 = tf.keras.layers.Dense(2)\n",
    "        self.sampl = Sampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flat1(x)\n",
    "        mu = self.dens1(x)\n",
    "        va = self.dens2(x)\n",
    "        z = self.sampl([mu,va])\n",
    "        \n",
    "        return mu, va, z\n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(32,32,1))\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dens3 = tf.keras.layers.Dense(units=4*4*128, activation=tf.nn.relu, input_shape=(2,))\n",
    "        self.resh1 = tf.keras.layers.Reshape(target_shape=(4, 4, 128))\n",
    "        self.deco1 = tf.keras.layers.Conv2DTranspose(64, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        self.deco2 = tf.keras.layers.Conv2DTranspose(32, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        # No activation\n",
    "        self.deco3 = tf.keras.layers.Conv2DTranspose(img_chn, 3, strides=2, padding='same')\n",
    "        \n",
    "    def call(self, inp):\n",
    "        x = self.dens3(inp)\n",
    "        x = self.resh1(x)\n",
    "        x = self.deco1(x)\n",
    "        x = self.deco2(x)\n",
    "        x = self.deco3(x)\n",
    "        return x\n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(2))\n",
    "        return Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(tf.keras.Model):\n",
    "    def __init__(self,):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def call(self, inp):\n",
    "        mu, va, z = self.encoder(inp)\n",
    "        out_decoder = self.decoder(z)\n",
    "        return z, out_decoder \n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(32,32,1))\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "    def encode(self, img):\n",
    "        return self.encoder(img)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AE() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2048)              6144      \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 8, 8, 64)         73792     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 16, 16, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 32, 32, 1)        289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 98,689\n",
      "Trainable params: 98,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.decoder.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training and testing sets are:\n",
      "X_train.shape: (50000, 32, 32, 1) & X_test.shape: (10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "img_chn = 1\n",
    "\n",
    "# Load CIFAR-10 dataset-\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "if img_chn == 1:\n",
    "    x_train = x_train.mean(axis=3)                                                                                    \n",
    "    x_test = x_test.mean(axis=3)           \n",
    "    \n",
    "input_shape = (img_rows, img_cols, img_chn)\n",
    "\n",
    "# Convert datasets to floating point types-\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize the training and testing datasets-\n",
    "if img_chn == 1:\n",
    "    x_train = x_train.reshape((x_train.shape[0], \\\n",
    "                                         img_rows, img_rows, img_chn)) / 255.\n",
    "    x_test = x_test.reshape((x_test.shape[0], \\\n",
    "                                      img_rows, img_rows, img_chn)) / 255.\n",
    "else:\n",
    "    x_train = x_train/255.\n",
    "    x_test = x_test/255.\n",
    "\n",
    "\n",
    "print(\"\\nDimensions of training and testing sets are:\")\n",
    "print(f\"X_train.shape: {x_train.shape} & X_test.shape: {x_test.shape}\")\n",
    "\n",
    "load_outlier_detector = True\n",
    "\n",
    "latent_dim = 2\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = model.encode(x_train[0].reshape(1, 32, 32, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model.decode(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f49c85099d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.imshow(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
