{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Input, Flatten,\\\n",
    "Conv2DTranspose, BatchNormalization, LeakyReLU, Reshape, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner\n",
    "import time\n",
    "# import plotly\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training and testing sets are:\n",
      "X_train.shape: (50000, 32, 32, 1) & X_test.shape: (10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "img_chn = 1\n",
    "\n",
    "# Load CIFAR-10 dataset-\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "if img_chn == 1:\n",
    "    x_train = x_train.mean(axis=3)                                                                                    \n",
    "    x_test = x_test.mean(axis=3)           \n",
    "    \n",
    "input_shape = (img_rows, img_cols, img_chn)\n",
    "\n",
    "# Convert datasets to floating point types-\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize the training and testing datasets-\n",
    "if img_chn == 1:\n",
    "    x_train = x_train.reshape((x_train.shape[0], \\\n",
    "                                         img_rows, img_rows, img_chn)) / 255.\n",
    "    x_test = x_test.reshape((x_test.shape[0], \\\n",
    "                                      img_rows, img_rows, img_chn)) / 255.\n",
    "else:\n",
    "    x_train = x_train/255.\n",
    "    x_test = x_test/255.\n",
    "\n",
    "\n",
    "print(\"\\nDimensions of training and testing sets are:\")\n",
    "print(f\"X_train.shape: {x_train.shape} & X_test.shape: {x_test.shape}\")\n",
    "\n",
    "load_outlier_detector = True\n",
    "\n",
    "latent_dim = 2\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_creation(X_data, batch_size):\n",
    "    end = X_data.shape[0]-1\n",
    "    start = 0\n",
    "    batch_split_X = list()\n",
    "    while start < end:\n",
    "        img_slice = np.array(X_data[start:start+batch_size])\n",
    "        batch_split_X.append(img_slice)\n",
    "        start = start + batch_size\n",
    "    return np.array(batch_split_X, dtype=object)\n",
    "    \n",
    "train_dataset = batch_creation(x_train, 32)\n",
    "test_dataset = batch_creation(x_test, 32)\n",
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_images(model, n, epoch, im_size=32, save=True, first_epoch=False, f_ep_count=0):\n",
    "    \n",
    "    # Create image matrix \n",
    "    image_width = im_size*n\n",
    "    image_height = image_width\n",
    "    image = np.zeros((image_height, image_width, img_chn))\n",
    "\n",
    "    # Create list of values which are evenly spaced wrt probability mass\n",
    "\n",
    "    norm = tfp.distributions.Normal(0, 1)\n",
    "    grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
    "    \n",
    "    # For each point on the grid in the latent space, decode and\n",
    "\n",
    "    # copy the image into the image array\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z = np.array([[xi, yi]])\n",
    "            x_decoded = model.sample(z)\n",
    "            digit = tf.reshape(x_decoded[0], (im_size, im_size, img_chn))\n",
    "            image[i * im_size: (i + 1) * im_size,\n",
    "                  j * im_size: (j + 1) * im_size] = digit.numpy()\n",
    "    \n",
    "\n",
    "    # Plot the image array\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image, cmap='Greys_r')\n",
    "    plt.axis('Off')\n",
    "\n",
    "\n",
    "#     # Potentially save, with different formatting if within first epoch\n",
    "#     if save and first_epoch:\n",
    "#         plt.savefig('tf_grid_at_epoch_{:04d}.{:04d}.png'.format(epoch, f_ep_count))\n",
    "#     elif save:\n",
    "#         plt.savefig('tf_grid_at_epoch_{:04d}.png'.format(epoch))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar, z = model.encode(x)\n",
    "    print(f'In compute loss')\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "#     print(f'Ending compute loss, {-tf.reduce_mean(logpx_z + logpz - logqz_x)}')\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "latent_dim = 2\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "img_chn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,hp):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n",
    "        # adding filter size or kernel size\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\\\n",
    "        #activation function\n",
    "        activation='relu',input_shape=(32,32,1))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.flat1 = tf.keras.layers.Flatten()\n",
    "        self.dens1 = tf.keras.layers.Dense(2)\n",
    "        self.dens2 = tf.keras.layers.Dense(2)\n",
    "        self.sampl = Sampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flat1(x)\n",
    "        mu = self.dens1(x)\n",
    "        va = self.dens2(x)\n",
    "        z = self.sampl([mu,va])\n",
    "        \n",
    "        return mu, va, z\n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(32,32,1))\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dens3 = tf.keras.layers.Dense(units=4*4*128, activation=tf.nn.relu, input_shape=(2,))\n",
    "        self.resh1 = tf.keras.layers.Reshape(target_shape=(4, 4, 128))\n",
    "        self.deco1 = tf.keras.layers.Conv2DTranspose(64, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        self.deco2 = tf.keras.layers.Conv2DTranspose(32, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        # No activation\n",
    "        self.deco3 = tf.keras.layers.Conv2DTranspose(img_chn, 3, strides=2, padding='same')\n",
    "        \n",
    "    def call(self, inp):\n",
    "        x = self.dens3(inp)\n",
    "        x = self.resh1(x)\n",
    "        x = self.deco1(x)\n",
    "        x = self.deco2(x)\n",
    "        x = self.deco3(x)\n",
    "        return x\n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(2))\n",
    "        return Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(tf.keras.Model):\n",
    "    def __init__(self,hp):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = Encoder(hp)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def call(self, inp):\n",
    "        mu, va, z = self.encoder(inp)\n",
    "        out_decoder = self.decoder(z)\n",
    "        return z, out_decoder \n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(32,32,1))\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "    def encode(self, img):\n",
    "        return self.encoder(img)\n",
    "    \n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "    \n",
    "    @tf.function\n",
    "    def sample(self, z=None):\n",
    "        if z is None:\n",
    "            z = tf.random.normal(shape=(100, latent_dim))\n",
    "        return self.decode(z, apply_sigmoid=True)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, x, optimizer):\n",
    "        \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "        This function computes the loss and gradients, and uses the latter to\n",
    "        update the model's parameters.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = compute_loss(model, x)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE_Hypermodel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        return AE(hp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h_model = CVAE_Hypermodel()\n",
    "hp = keras_tuner.HyperParameters()\n",
    "model = h_model.build(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In compute loss\n",
      "In compute loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-39a7583800ff>:27: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure(figsize=(10, 10))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "In compute loss\n",
      "Epoch: 1, Test set ELBO: -662.1259155273438, time elapse for current epoch: 15.487292289733887\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, train_x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m75\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     13\u001b[0m         plot_latent_images(model, \u001b[38;5;241m10\u001b[39m, epoch\u001b[38;5;241m=\u001b[39mepoch, first_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, f_ep_count\u001b[38;5;241m=\u001b[39midx)          \n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "tf.config.run_functions_eagerly(False)\n",
    "plot_latent_images(model, 10, epoch=0)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    for idx, train_x in enumerate(train_dataset):\n",
    "        model.train_step(train_x, optimizer)\n",
    "        if epoch == 1 and idx % 75 == 0:\n",
    "            plot_latent_images(model, 10, epoch=epoch, first_epoch=True, f_ep_count=idx)          \n",
    "    end_time = time.time()\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    for test_x in test_dataset:\n",
    "        loss(compute_loss(model, test_x))\n",
    "    elbo = -loss.result()\n",
    "    #display.clear_output(wait=False)\n",
    "    print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
    "        .format(epoch, elbo, end_time - start_time))\n",
    "    if epoch != 1:\n",
    "        plot_latent_images(model, 20, epoch=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
