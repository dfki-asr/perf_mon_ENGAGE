{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Input, Flatten,\\\n",
    "Conv2DTranspose, BatchNormalization, LeakyReLU, Reshape, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner\n",
    "import time\n",
    "# import plotly\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "latent_dim = 2\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "img_chn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VAE\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 16, 16, 32)   320         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 8, 8, 64)     18496       ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 4, 4, 128)    73856       ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 2048)         0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 2)            4098        ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 2)            4098        ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_layer (Sampling)      (None, 2)            0           ['dense_6[0][0]',                \n",
      "                                                                  'dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 2048)         6144        ['sampling_layer[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 4, 4, 128)    0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 8, 8, 64)    73792       ['reshape_2[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 16, 16, 32)  18464       ['conv2d_transpose_6[0][0]']     \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_transpose_8 (Conv2DTran  (None, 32, 32, 1)   289         ['conv2d_transpose_7[0][0]']     \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 199,557\n",
      "Trainable params: 199,557\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = keras.Input(shape=(32, 32, 1))\n",
    "#first conv\n",
    "x =  Conv2D(32, 3, strides=2, padding='same', activation=tf.nn.relu)(encoder_inputs)\n",
    "x =  Conv2D(64, 3, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "x =  Conv2D(128, 3, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "x = Flatten()(x)\n",
    "z_mean = Dense(latent_dim )(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "z = Sampling(name='sampling_layer')([z_mean, z_log_var])\n",
    "\n",
    "# self.encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "#         encoder.summary()\n",
    "\n",
    "# latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = Dense(units=4*4*128, activation=tf.nn.relu)(z)\n",
    "x = Reshape(target_shape=(4, 4, 128))(x)\n",
    "x = Conv2DTranspose(64, 3, strides=2, padding='same',  activation=tf.nn.relu)(x)\n",
    "x = Conv2DTranspose(32, 3, strides=2, padding='same',  activation=tf.nn.relu)(x)\n",
    "dec_out = Conv2DTranspose(img_chn, 3, strides=2, padding='same')(x)\n",
    "\n",
    "\n",
    "#         decoder.summary()\n",
    "vae_model = Model(encoder_inputs, dec_out, name=\"VAE\")\n",
    "vae_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.flat1 = tf.keras.layers.Flatten()\n",
    "        self.dens1 = tf.keras.layers.Dense(2)\n",
    "        self.dens2 = tf.keras.layers.Dense(2)\n",
    "        self.sampl = Sampling()\n",
    "        self.decIn = tf.keras.layers.InputLayer(input_shape=(2,))\n",
    "        self.dens3 = tf.keras.layers.Dense(units=4*4*128, activation=tf.nn.relu)\n",
    "        self.resh1 = tf.keras.layers.Reshape(target_shape=(4, 4, 128))\n",
    "        self.deco1 = tf.keras.layers.Conv2DTranspose(64, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        self.deco2 = tf.keras.layers.Conv2DTranspose(32, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        # No activation\n",
    "        self.deco3 = tf.keras.layers.Conv2DTranspose(img_chn, 3, strides=2, padding='same')\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flat1(x)\n",
    "        mu = self.dens1(x)\n",
    "        va = self.dens2(x)\n",
    "        z = self.sampl([mu,va])\n",
    "        x = self.decIn(z)\n",
    "        x = self.dens3(x)\n",
    "        x = self.resh1(x)\n",
    "        x = self.deco1(x)\n",
    "        x = self.deco2(x)\n",
    "        x = self.deco3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.build(input_shape=(None,32,32,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation=tf.nn.relu, input_shape=(32,32,1))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, 3, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        self.flat1 = tf.keras.layers.Flatten()\n",
    "        self.dens1 = tf.keras.layers.Dense(2)\n",
    "        self.dens2 = tf.keras.layers.Dense(2)\n",
    "        self.sampl = Sampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flat1(x)\n",
    "        mu = self.dens1(x)\n",
    "        va = self.dens2(x)\n",
    "        z = self.sampl([mu,va])\n",
    "        \n",
    "        return mu, va, z\n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(32,32,1))\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dens3 = tf.keras.layers.Dense(units=4*4*128, activation=tf.nn.relu, input_shape=(2,))\n",
    "        self.resh1 = tf.keras.layers.Reshape(target_shape=(4, 4, 128))\n",
    "        self.deco1 = tf.keras.layers.Conv2DTranspose(64, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        self.deco2 = tf.keras.layers.Conv2DTranspose(32, 3, strides=2, padding='same',  activation=tf.nn.relu)\n",
    "        # No activation\n",
    "        self.deco3 = tf.keras.layers.Conv2DTranspose(img_chn, 3, strides=2, padding='same')\n",
    "        \n",
    "    def call(self, inp):\n",
    "        x = self.dens3(inp)\n",
    "        x = self.resh1(x)\n",
    "        x = self.deco1(x)\n",
    "        x = self.deco2(x)\n",
    "        x = self.deco3(x)\n",
    "        return x\n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(2))\n",
    "        return Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(tf.keras.Model):\n",
    "    def __init__(self,):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def call(self, inp):\n",
    "        mu, va, z = self.encoder(inp)\n",
    "        out_decoder = self.decoder(z)\n",
    "        return z, out_decoder \n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(32,32,1))\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "    def encode(self, img):\n",
    "        return self.encoder(img)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AE() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_25 (InputLayer)       [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 2048)              6144      \n",
      "                                                                 \n",
      " reshape_13 (Reshape)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_39 (Conv2D  (None, 8, 8, 64)         73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_40 (Conv2D  (None, 16, 16, 32)       18464     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_41 (Conv2D  (None, 32, 32, 1)        289       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 98,689\n",
      "Trainable params: 98,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.decoder.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training and testing sets are:\n",
      "X_train.shape: (50000, 32, 32, 1) & X_test.shape: (10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "img_chn = 1\n",
    "\n",
    "# Load CIFAR-10 dataset-\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "if img_chn == 1:\n",
    "    x_train = x_train.mean(axis=3)                                                                                    \n",
    "    x_test = x_test.mean(axis=3)           \n",
    "    \n",
    "input_shape = (img_rows, img_cols, img_chn)\n",
    "\n",
    "# Convert datasets to floating point types-\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize the training and testing datasets-\n",
    "if img_chn == 1:\n",
    "    x_train = x_train.reshape((x_train.shape[0], \\\n",
    "                                         img_rows, img_rows, img_chn)) / 255.\n",
    "    x_test = x_test.reshape((x_test.shape[0], \\\n",
    "                                      img_rows, img_rows, img_chn)) / 255.\n",
    "else:\n",
    "    x_train = x_train/255.\n",
    "    x_test = x_test/255.\n",
    "\n",
    "\n",
    "print(\"\\nDimensions of training and testing sets are:\")\n",
    "print(f\"X_train.shape: {x_train.shape} & X_test.shape: {x_test.shape}\")\n",
    "\n",
    "load_outlier_detector = True\n",
    "\n",
    "latent_dim = 2\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = model.encode(x_train[0].reshape(1, 32, 32, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model.decode(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb468c56a60>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
