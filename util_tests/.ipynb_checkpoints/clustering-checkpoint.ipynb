{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Input, Flatten,\\\n",
    "Conv2DTranspose, BatchNormalization, LeakyReLU, Reshape, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.datasets import cifar10, mnist\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner\n",
    "import time\n",
    "# import plotly\n",
    "# import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "# Load CIFAR-10 dataset-\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n",
    "    input_shape = (img_rows, img_cols, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyper-parameters-\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of training and testing sets are:\n",
      "X_train.shape: (50000, 32, 32, 3) & X_test.shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert datasets to floating point types-\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize the training and testing datasets-\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "print(\"\\nDimensions of training and testing sets are:\")\n",
    "print(f\"X_train.shape: {X_train.shape} & X_test.shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# Create TF datasets-\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(50000).batch(batch_size = batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(X_test).shuffle(10000).batch(batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(Model):\n",
    "    def __init__(\n",
    "        self, num_filters,\n",
    "        kernel_size, stride_length,\n",
    "        pooling_size, pooling_stride,\n",
    "        padding_type = 'same'\n",
    "    ):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv2D(\n",
    "            filters = num_filters, kernel_size = kernel_size,\n",
    "            strides = stride_length, padding = padding_type,\n",
    "            activation = None, use_bias = False,\n",
    "        )\n",
    "        self.bn = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "        \n",
    "        self.conv2 = Conv2D(\n",
    "            filters = num_filters, kernel_size = kernel_size,\n",
    "            strides = stride_length, padding = padding_type,\n",
    "            activation = None, use_bias = False\n",
    "        )\n",
    "        self.bn2 = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "        \n",
    "        self.pool = MaxPooling2D(\n",
    "            pool_size = pooling_size,\n",
    "            strides = pooling_stride\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def call(self, x):\n",
    "        x = tf.keras.activations.relu(self.bn(self.conv1(x)))\n",
    "        x = tf.keras.activations.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv6_Encoder(Model):\n",
    "    def __init__(self, latent_dim = 10):\n",
    "        super(Conv6_Encoder, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.conv_block1 = ConvBlock(\n",
    "            num_filters = 64, kernel_size = 3,\n",
    "            stride_length = 1, pooling_size = 2,\n",
    "            pooling_stride = 2, padding_type = 'valid'\n",
    "            )\n",
    "\n",
    "        self.conv_block2 = ConvBlock(\n",
    "            num_filters = 128, kernel_size = 3,\n",
    "            stride_length = 1, pooling_size = 2,\n",
    "            pooling_stride = 2, padding_type = 'valid'\n",
    "            )\n",
    "        \n",
    "        self.conv_block3 = ConvBlock(\n",
    "            num_filters = 256, kernel_size = 3,\n",
    "            stride_length = 1, pooling_size = 2,\n",
    "            pooling_stride = 2, padding_type = 'same'\n",
    "            )\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.output_layer = Dense(\n",
    "            units = self.latent_dim, activation = None\n",
    "            )\n",
    "        self.bn = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "        \n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = tf.keras.activations.relu(self.bn(self.output_layer(x)))\n",
    "        return x\n",
    "    \n",
    "    def model(self):\n",
    "        '''\n",
    "        Overrides 'model()' call.\n",
    "        Output shape is not well-defined when using sub-classing. As a\n",
    "        workaround, this method is implemeted.\n",
    "        '''\n",
    "        x = Input(shape = (32, 32, 3))\n",
    "        return Model(inputs = [x], outputs = self.call(x))\n",
    "    \n",
    "    def shape_computation(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        x = self.conv_block1(x)\n",
    "        print(f\"conv_block1.shape: {x.shape}\")\n",
    "        x = self.conv_block2(x)\n",
    "        print(f\"conv_block2.shape: {x.shape}\")\n",
    "        x = self.conv_block3(x)\n",
    "        print(f\"conv_block3.shape: {x.shape}\")\n",
    "        x = self.flatten(x)\n",
    "        print(f\"flattened shape: {x.shape}\")\n",
    "        x = tf.keras.activations.relu(self.bn(self.output_layer(x)))\n",
    "        print(f\"Encoder output shape: {x.shape}\")\n",
    "        '''\n",
    "        Input shape: (64, 32, 32, 3)\n",
    "        conv_block1.shape: (64, 14, 14, 64)\n",
    "        conv_block2.shape: (64, 5, 5, 128)\n",
    "        conv_block3.shape: (64, 2, 2, 256)\n",
    "        flattened shape: (64, 1024)\n",
    "        Encoder output shape: (64, 100)\n",
    "        '''\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv6_Decoder(Model):\n",
    "    def __init__(self, latent_dim = 10):\n",
    "        super(Conv6_Decoder, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # self.inp_layer = InputLayer(input_shape = self.latent_dim)\n",
    "        \n",
    "        self.dense0 = Dense(\n",
    "            units = self.latent_dim, activation = None\n",
    "            )\n",
    "        self.bn0 = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "        \n",
    "        self.dense = Dense(\n",
    "            units = 1024, activation = None\n",
    "        )\n",
    "        self.bn = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "        \n",
    "        self.dense2 = Dense(\n",
    "            units = 4 * 4 * 256, activation = None\n",
    "        )\n",
    "        self.bn2 = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "        \n",
    "        self.reshape = Reshape((4, 4, 256))\n",
    "        \n",
    "        self.conv_transpose_layer1 = Conv2DTranspose(\n",
    "            filters = 256, kernel_size = 3,\n",
    "            strides = 2, padding = 'same',\n",
    "            activation = None\n",
    "            )\n",
    "        self.bn3 = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "       \n",
    "        self.conv_transpose_layer2 = Conv2DTranspose(\n",
    "            filters = 256, kernel_size = 3,\n",
    "            strides = 1, padding = 'same',\n",
    "            activation = None\n",
    "            )\n",
    "        \n",
    "        self.bn4 = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "        \n",
    "        self.conv_transpose_layer3 =  Conv2DTranspose(\n",
    "            filters = 128, kernel_size = 3,\n",
    "            strides = 2, padding = 'same',\n",
    "            activation = None\n",
    "            )\n",
    "        self.bn5 = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "        \n",
    "        self.conv_transpose_layer4 = Conv2DTranspose(\n",
    "            filters = 128, kernel_size = 3,\n",
    "            strides = 1, padding = 'same',\n",
    "            activation = None\n",
    "            )\n",
    "        \n",
    "        self.bn6 = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "\n",
    "        self.conv_transpose_layer5 = Conv2DTranspose(\n",
    "            filters = 64, kernel_size = 3,\n",
    "            strides = 2, padding = 'same',\n",
    "            activation = None\n",
    "            )\n",
    "        self.bn7 = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "       \n",
    "        self.conv_transpose_layer6 = Conv2DTranspose(\n",
    "            filters = 64, kernel_size = 3,\n",
    "            strides = 1, padding = 'same',\n",
    "            activation = None\n",
    "            )\n",
    "        \n",
    "        self.bn8 = BatchNormalization(\n",
    "            axis = -1, momentum = 0.99,\n",
    "            epsilon = 0.001\n",
    "            )\n",
    "        \n",
    "        self.final_conv_layer = Conv2DTranspose(\n",
    "            filters = 3, kernel_size = 3,\n",
    "            strides = 1, padding = 'same',\n",
    "            activation = None\n",
    "            )\n",
    "        \n",
    "    \n",
    "    def call(self, X):\n",
    "        # X = self.inp_layer(X)\n",
    "        X = tf.keras.activations.relu(self.bn0(self.dense0(X)))\n",
    "        X = tf.keras.activations.relu(self.bn(self.dense(X)))\n",
    "        X = tf.keras.activations.relu(self.bn2(self.dense2(X)))\n",
    "        X = self.reshape(X)\n",
    "        X = tf.keras.activations.relu(self.bn3(self.conv_transpose_layer1(X)))\n",
    "        X = tf.keras.activations.relu(self.bn4(self.conv_transpose_layer2(X)))\n",
    "        X = tf.keras.activations.relu(self.bn5(self.conv_transpose_layer3(X)))\n",
    "        X = tf.keras.activations.relu(self.bn6(self.conv_transpose_layer4(X)))\n",
    "        X = tf.keras.activations.relu(self.bn7(self.conv_transpose_layer5(X)))\n",
    "        X = tf.keras.activations.relu(self.bn8(self.conv_transpose_layer6(X)))\n",
    "        # X = tf.keras.activations.sigmoid(self.final_conv_layer(X))\n",
    "        X = self.final_conv_layer(X)\n",
    "\n",
    "        return X\n",
    "        \n",
    "    def model(self):\n",
    "        '''\n",
    "        Overrides 'model()' call.\n",
    "        Output shape is not well-defined when using sub-classing. As a\n",
    "        workaround, this method is implemeted.\n",
    "        '''\n",
    "        x = Input(shape = (100))\n",
    "        return Model(inputs = [x], outputs = self.call(x))\n",
    "    \n",
    "    def shape_computation(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        x = tf.nn.relu(self.bn0(self.dense0(x)))\n",
    "        print(f\"first dense layer shape: {x.shape}\")\n",
    "        x = tf.nn.relu(self.bn(self.dense(x)))\n",
    "        print(f\"second dense layer shape: {x.shape}\")\n",
    "        x = tf.nn.relu(self.bn2(self.dense2(x)))\n",
    "        print(f\"third dense layer shape: {x.shape}\")\n",
    "        x = self.reshape(x)\n",
    "        print(f\"reshape: {x.shape}\")\n",
    "        x = tf.nn.relu(self.bn3(self.conv_transpose_layer1(x)))\n",
    "        print(f\"conv transpose layer1 shape: {x.shape}\")\n",
    "        x = tf.nn.relu(self.bn4(self.conv_transpose_layer2(x)))\n",
    "        print(f\"conv transpose layer2 shape: {x.shape}\")\n",
    "        x = tf.nn.relu(self.bn5(self.conv_transpose_layer3(x)))\n",
    "        print(f\"conv transpose layer3 shape: {x.shape}\")\n",
    "        x = tf.nn.relu(self.bn6(self.conv_transpose_layer4(x)))\n",
    "        print(f\"conv transpose layer4 shape: {x.shape}\")\n",
    "        x = tf.nn.relu(self.bn7(self.conv_transpose_layer5(x)))\n",
    "        print(f\"conv transpose layer5 shape: {x.shape}\")\n",
    "        x = tf.nn.relu(self.bn8(self.conv_transpose_layer6(x)))\n",
    "        print(f\"conv transpose layer6 shape: {x.shape}\")\n",
    "        x = self.final_conv_layer(x)\n",
    "        print(f\"Decoder output shape: {x.shape}\")\n",
    "        \n",
    "        '''\n",
    "        Input shape: (64, 100)\n",
    "        first dense layer shape: (64, 100)\n",
    "        second dense layer shape: (64, 1024)\n",
    "        third dense layer shape: (64, 4096)\n",
    "        reshape: (64, 4, 4, 256)\n",
    "        conv transpose layer1 shape: (64, 8, 8, 256)\n",
    "        conv transpose layer2 shape: (64, 8, 8, 256)\n",
    "        conv transpose layer3 shape: (64, 16, 16, 128)\n",
    "        conv transpose layer4 shape: (64, 16, 16, 128)\n",
    "        conv transpose layer5 shape: (64, 32, 32, 64)\n",
    "        conv transpose layer6 shape: (64, 32, 32, 64)\n",
    "        Decoder output shape: (64, 32, 32, 3)\n",
    "        '''\n",
    "        \n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Create a sampling layer.\n",
    "    Uses (mu, log_var) to sample latent vector 'z'.\n",
    "    \"\"\"\n",
    "    def call(self, mu, log_var):\n",
    "    # def call(self, inputs):\n",
    "        # z_mean, z_log_var = inputs\n",
    "\n",
    "        # Get batch size-\n",
    "        batch = tf.shape(mu)[0]\n",
    "\n",
    "        # Get latent space dimensionality-\n",
    "        dim = tf.shape(mu)[1]\n",
    "\n",
    "        # Add stochasticity by sampling from a multivariate standard \n",
    "        # Gaussian distribution-\n",
    "        epsilon = tf.keras.backend.random_normal(\n",
    "            shape = (batch, dim), mean = 0.0,\n",
    "            stddev = 1.0\n",
    "        )\n",
    "\n",
    "        return mu + (tf.exp(0.5 * log_var) * epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, latent_space = 100):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.latent_space = latent_space\n",
    "        \n",
    "        self.encoder = Conv6_Encoder(latent_dim = self.latent_space)\n",
    "        self.decoder = Conv6_Decoder(latent_dim = self.latent_space)\n",
    "        \n",
    "        # Define fully-connected layers for computing mean & log variance-\n",
    "        self.mu = Dense(units = self.latent_space, activation = None)\n",
    "        self.log_var = Dense(units = self.latent_space, activation = None)\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Sample from a multivariate Gaussian distribution.\n",
    "        # Adds stochasticity or variation-\n",
    "        eps = tf.random.normal(\n",
    "            shape = mu.shape, mean = 0.0,\n",
    "            stddev = 1.0\n",
    "        )\n",
    "        return (eps * tf.exp(logvar * 0.5) + mu)\n",
    "        \n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        # print(f\"x.shape: {x.shape}\")\n",
    "        # x.shape: (batch_size, 100)\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        log_var = self.log_var(x)\n",
    "        # z = self.reparameterize(mu, log_var)\n",
    "        # z = Sampling()([mu, log_var])\n",
    "        z = Sampling()(mu, log_var)\n",
    "        '''\n",
    "        print(f\"mu.shape: {mu.shape}, log_var.shape: {log_var.shape}\"\n",
    "              f\" & z.shape: {z.shape}\")\n",
    "        # mu.shape: (batch_size, 100), log_var.shape: (batch_size, 100) & z.shape: (batch_size, 100)\n",
    "        '''\n",
    "        \n",
    "        x = tf.keras.activations.sigmoid(self.decoder(z))\n",
    "        return x, mu, log_var, z\n",
    "    \n",
    "    \n",
    "    def model(self):\n",
    "        '''\n",
    "        Overrides 'model()' call.\n",
    "        Output shape is not well-defined when using sub-classing. As a\n",
    "        workaround, this method is implemeted.\n",
    "        '''\n",
    "        x = Input(shape = (32, 32, 3))\n",
    "        return Model(inputs = [x], outputs = self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, latent_space = 100):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.latent_space = latent_space\n",
    "        \n",
    "        self.encoder = Conv6_Encoder(latent_dim = self.latent_space)\n",
    "        self.decoder = Conv6_Decoder(latent_dim = self.latent_space)\n",
    "        \n",
    "        # Define fully-connected layers for computing mean & log variance-\n",
    "        self.mu = Dense(units = self.latent_space, activation = None)\n",
    "        self.log_var = Dense(units = self.latent_space, activation = None)\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Sample from a multivariate Gaussian distribution.\n",
    "        # Adds stochasticity or variation-\n",
    "        eps = tf.random.normal(\n",
    "            shape = mu.shape, mean = 0.0,\n",
    "            stddev = 1.0\n",
    "        )\n",
    "        return (eps * tf.exp(logvar * 0.5) + mu)\n",
    "        \n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        # print(f\"x.shape: {x.shape}\")\n",
    "        # x.shape: (batch_size, 100)\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        log_var = self.log_var(x)\n",
    "        # z = self.reparameterize(mu, log_var)\n",
    "        # z = Sampling()([mu, log_var])\n",
    "        z = Sampling()(mu, log_var)\n",
    "        '''\n",
    "        print(f\"mu.shape: {mu.shape}, log_var.shape: {log_var.shape}\"\n",
    "              f\" & z.shape: {z.shape}\")\n",
    "        # mu.shape: (batch_size, 100), log_var.shape: (batch_size, 100) & z.shape: (batch_size, 100)\n",
    "        '''\n",
    "        \n",
    "        x = tf.keras.activations.sigmoid(self.decoder(z))\n",
    "        return x, mu, log_var, z\n",
    "    \n",
    "    \n",
    "    def model(self):\n",
    "        '''\n",
    "        Overrides 'model()' call.\n",
    "        Output shape is not well-defined when using sub-classing. As a\n",
    "        workaround, this method is implemeted.\n",
    "        '''\n",
    "        x = Input(shape = (32, 32, 3))\n",
    "        return Model(inputs = [x], outputs = self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv6__encoder (Conv6_Encoder)  (None, 100)         1250996     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 100)          10100       ['conv6__encoder[0][0]']         \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 100)          10100       ['conv6__encoder[0][0]']         \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 100)          0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " conv6__decoder (Conv6_Decoder)  (None, 32, 32, 3)   6071623     ['sampling[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid (TFOpLambda)   (None, 32, 32, 3)    0           ['conv6__decoder[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,342,819\n",
      "Trainable params: 7,328,595\n",
      "Non-trainable params: 14,224\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize VAE model-\n",
    "model = VAE(latent_space = 100)\n",
    "# Get model summary-\n",
    "model.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimizer-\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(data, reconstruction, mu, log_var, alpha = 1):\n",
    "    \n",
    "    # Reconstruction loss-\n",
    "    # recon_loss = tf.keras.losses.mean_squared_error(K.flatten(data), K.flatten(reconstruction))\n",
    "\n",
    "    recon_loss = tf.reduce_mean(\n",
    "        tf.reduce_sum(\n",
    "            # tf.keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "            tf.keras.losses.mean_squared_error(data, reconstruction),\n",
    "            axis = (1, 2)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # KL-divergence loss-    \n",
    "    kl_loss = -0.5 * (1 + log_var - tf.square(mu) - tf.exp(log_var))\n",
    "    kl_loss = tf.reduce_mean(\n",
    "        tf.reduce_sum(\n",
    "            kl_loss,\n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    total_loss = (recon_loss * alpha) + kl_loss\n",
    "    \n",
    "    return total_loss, recon_loss, kl_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_one_step(model, optimizer, data, alpha):\n",
    "    # Function to perform one step/iteration of training\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make predictions using defined model-\n",
    "        data_recon, mu, log_var, z = model(data)\n",
    "\n",
    "        # Compute loss-\n",
    "        total_loss, recon_loss, kl_loss = compute_loss(\n",
    "            data = data, reconstruction = data_recon,\n",
    "            mu = mu, log_var = log_var,\n",
    "            alpha = alpha\n",
    "        )\n",
    "    \n",
    "    # Compute gradients wrt defined loss and weights and biases-\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    \n",
    "    # type(grads)\n",
    "    # list\n",
    "    \n",
    "    # Apply computed gradients to model's weights and biases-\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, optimizer, data, alpha):\n",
    "    '''\n",
    "    Function to test model performance\n",
    "    on testing dataset\n",
    "    '''\n",
    "    # Make predictions using defined model-\n",
    "    data_recon, mu, log_var, z = model(data)\n",
    "    \n",
    "    # Compute loss-\n",
    "    total_loss, recon_loss, kl_loss = compute_loss(\n",
    "        data = data, reconstruction = data_recon,\n",
    "        mu = mu, log_var = log_var,\n",
    "        alpha = alpha\n",
    "    )\n",
    "    \n",
    "    return total_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyper-parameter for reconstruction loss vs. kl-divergence-\n",
    "alpha = 10\n",
    "\n",
    "# Python3 dict to contain training metrics-\n",
    "training_metrics = {}\n",
    "val_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1; total train loss = 36750.4284, train recon loss = 3673.5299, train kl loss = 15.1291; total val loss = 0.1206, val recon loss = 0.0120 & val kl loss = 0.0006\n",
      "epoch = 2; total train loss = 0.2376, train recon loss = 0.0237, train kl loss = 0.0007; total val loss = 0.0173, val recon loss = 0.0017 & val kl loss = 0.0001\n",
      "epoch = 3; total train loss = 0.0506, train recon loss = 0.0050, train kl loss = 0.0004; total val loss = 0.0060, val recon loss = 0.0006 & val kl loss = 0.0001\n",
      "epoch = 4; total train loss = 0.0175, train recon loss = 0.0017, train kl loss = 0.0003; total val loss = 0.0023, val recon loss = 0.0002 & val kl loss = 0.0001\n",
      "epoch = 5; total train loss = 0.0078, train recon loss = 0.0008, train kl loss = 0.0002; total val loss = 0.0011, val recon loss = 0.0001 & val kl loss = 0.0001\n",
      "epoch = 6; total train loss = 0.0037, train recon loss = 0.0004, train kl loss = 0.0002; total val loss = 0.0005, val recon loss = 0.0001 & val kl loss = -0.0000\n",
      "epoch = 7; total train loss = 0.0023, train recon loss = 0.0002, train kl loss = 0.0002; total val loss = 0.0004, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 8; total train loss = 0.0017, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0003, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 9; total train loss = 0.0013, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 10; total train loss = 0.0011, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 11; total train loss = 0.0010, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 12; total train loss = 0.0009, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 13; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 14; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 15; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 16; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 17; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 18; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 19; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 20; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 21; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 22; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 23; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 24; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 25; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 26; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 27; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 28; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 29; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 30; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 31; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 32; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 33; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 34; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 35; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 36; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 37; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 38; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 39; total train loss = 0.0009, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 40; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 41; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 42; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 43; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 44; total train loss = 0.0011, train recon loss = 0.0001, train kl loss = 0.0006; total val loss = 0.0003, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 45; total train loss = 0.0009, train recon loss = 0.0001, train kl loss = 0.0004; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 46; total train loss = 0.0009, train recon loss = 0.0001, train kl loss = 0.0004; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 47; total train loss = 0.0010, train recon loss = 0.0001, train kl loss = 0.0004; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 48; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 49; total train loss = 0.0013, train recon loss = 0.0001, train kl loss = 0.0007; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 50; total train loss = 0.0007, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 51; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 52; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 53; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 54; total train loss = 0.0007, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 55; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 56; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 57; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 58; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 59; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0003, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 60; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 61; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 62; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 63; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 64; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 65; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 66; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 67; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 68; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 69; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 70; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 71; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0003, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 72; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 73; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 74; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 75; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 76; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 77; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 78; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 79; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 80; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 81; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 82; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 83; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 84; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 85; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 86; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 87; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 88; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 89; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 90; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 91; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 92; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 93; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 94; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = -0.0000\n",
      "epoch = 95; total train loss = 0.0007, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 96; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 97; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 98; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0002, val recon loss = 0.0000 & val kl loss = 0.0001\n",
      "epoch = 99; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0002; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n",
      "epoch = 100; total train loss = 0.0008, train recon loss = 0.0001, train kl loss = 0.0003; total val loss = 0.0001, val recon loss = 0.0000 & val kl loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    \"\"\"\n",
    "    # Manual early stopping implementation-\n",
    "    if loc_patience >= patience:\n",
    "        print(\"\\n'EarlyStopping' called!\\n\")\n",
    "        break\n",
    "    \"\"\"\n",
    "\n",
    "    # Epoch train & validation losses-\n",
    "    train_loss = 0.0\n",
    "    train_r_loss = 0.0\n",
    "    train_kl_l = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_r_loss = 0.0\n",
    "    val_kl_l = 0.0\n",
    "    \n",
    "    for data in train_dataset:\n",
    "        train_total_loss, train_recon_loss, train_kl_loss = train_one_step(\n",
    "            model = model, optimizer = optimizer,\n",
    "            data = data, alpha = alpha\n",
    "        )\n",
    "        \n",
    "        train_loss += train_total_loss.numpy()\n",
    "        train_r_loss += train_recon_loss.numpy()\n",
    "        train_kl_l += train_kl_loss.numpy()\n",
    "    \n",
    "    for test_data in test_dataset:\n",
    "        test_total_loss, test_recon_loss, test_kl_loss = test_step(\n",
    "            model = model, optimizer = optimizer,\n",
    "            data = test_data, alpha = alpha)\n",
    "        \n",
    "        val_loss += test_total_loss.numpy()\n",
    "        val_r_loss += test_recon_loss.numpy()\n",
    "        val_kl_l += test_kl_loss.numpy()\n",
    "    \n",
    "    # vae_train_loss.append(train_loss)\n",
    "    # vae_val_loss.append(val_loss)\n",
    "\n",
    "    training_metrics[epoch] = {\n",
    "        'total_loss': train_loss, 'recon_loss': train_r_loss,\n",
    "        'kl_loss': train_kl_l\n",
    "        }\n",
    "    \n",
    "    val_metrics[epoch] = {\n",
    "        'total_loss': val_loss, 'recon_loss': val_r_loss,\n",
    "        'kl_loss': val_kl_l\n",
    "    }\n",
    "\n",
    "    print(f\"epoch = {epoch}; total train loss = {train_loss:.4f},\"\n",
    "    f\" train recon loss = {train_r_loss:.4f}, train kl loss = {train_kl_l:.4f};\"\n",
    "    f\" total val loss = {val_loss:.4f}, val recon loss = {val_r_loss:.4f} &\"\n",
    "    f\" val kl loss = {val_kl_l:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = X_test[:2000]\n",
    "y_data = y_test[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat, mu, log_var, z = model(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 24, 20, 14, 18, 21, 13, 20, 24, 23],\n",
       "       [25, 15, 33, 13, 21, 20, 22, 19, 14, 16],\n",
       "       [24, 19, 20, 13, 24, 25, 20, 22, 16, 12],\n",
       "       [27, 20, 27, 20, 15, 20, 19, 19, 22, 10],\n",
       "       [16, 16, 27, 27, 20, 15, 22, 16, 24, 15],\n",
       "       [20, 22, 23, 16, 15, 18, 14, 25, 19, 13],\n",
       "       [22, 23, 13, 26, 24, 17, 18, 29, 23, 21],\n",
       "       [21, 23, 27, 18, 15, 13, 27, 23, 15, 11],\n",
       "       [21, 26, 22, 19, 21, 23, 23, 27, 15, 20],\n",
       "       [10, 20, 25, 23, 28, 21, 21, 20, 15, 20]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(y_data, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "znp = z.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(2)\n",
    " \n",
    "#Transform the data\n",
    "df = pca.fit_transform(znp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
